{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad16860-884c-40fb-b24f-b23e4994e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ot\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f88de5-95f5-4cbb-a222-036c40244dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from HierarchicalOT import load_wmd_data,change_embeddings,reduce_vocab,fit_topics,sparseOT\n",
    "from sinkhorn_iterates import sinkhorn\n",
    "from linear_solver import UOT_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd28d9-e7be-4354-8f64-6de34bde7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/WordMoverDistances/'\n",
    "embeddings_path = './data/WordMoverDistances/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# Pick a dataset (n_doc,n_vocab)\n",
    "#data_name = 'bbcsport-emd_tr_te_split.mat' #(737,3657)\n",
    "data_name = 'twitter-emd_tr_te_split.mat' #(3108, 1205)\n",
    "#data_name = 'r8-emd_tr_te3.mat' # (7674,5495)\n",
    "#data_name = 'amazon-emd_tr_te_split.mat' #(8000, 16753)\n",
    "#data_name = 'classic-emd_tr_te_split.mat' # (7093, 5813)\n",
    "#data_name = 'ohsumed-emd_tr_te_ix.mat'# (9152, 8261)\n",
    "\n",
    "vocab, embed_vocab, bow_data, y = load_wmd_data(data_path + data_name)\n",
    "y = y - 1\n",
    "vocab, embed_vocab, bow_data = change_embeddings(vocab, bow_data, embeddings_path) # embed in Glove\n",
    "\n",
    "vocab, embed_vocab, bow_data = reduce_vocab(bow_data, vocab, embed_vocab, embed_aggregate='mean') # Reduce vocabulary by removing short words, stop words, and stemming (root words?)\n",
    "embeddings = np.array([embed_vocab[w] for w in vocab])\n",
    "cost_embeddings = euclidean_distances(embeddings, embeddings) ** 2 # Matrix of word embeddings: nb_vocab x 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d76232-e350-46a4-bd56-199ecc71f0f3",
   "metadata": {},
   "source": [
    "# WMD Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf008f-2951-4ad2-99f2-8548e373f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UOT penalization\n",
    "WMD=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        #print(j,end=\" \")\n",
    "        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "        WMD[i,j]=ot.emd2(a,b,C)\n",
    "WMD = WMD + WMD.T\n",
    "np.savetxt(\"result/WordMoverDistances/WMD_\"+data_name+\".txt\",WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6141a23-0c24-4dcf-94f7-3168ebe49086",
   "metadata": {},
   "source": [
    "# HOTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a289f82-c26b-4e29-b257-20b279e74ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, lda_centers, topic_proportions = fit_topics(bow_data, embeddings, vocab, K=70)\n",
    "\n",
    "# Reduce topics to top-20 words via threshold\n",
    "# topics is now sparse\n",
    "n_words_keep = 20\n",
    "if n_words_keep is not None:\n",
    "    for k in range(70):\n",
    "        to_0_idx = np.argsort(-topics[k])[n_words_keep:]\n",
    "        topics[k][to_0_idx] = 0\n",
    "        \n",
    "# Compute WD for the unormalized topics[i],topics[j]. \n",
    "# It then defines cost matrix for distrib on topics\n",
    "cost_topics = np.zeros((topics.shape[0], topics.shape[0]))        \n",
    "for i in range(cost_topics.shape[0]):\n",
    "    for j in range(i + 1, cost_topics.shape[1]):\n",
    "        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "        cost_topics[i,j]=ot.emd2(a,b,C)\n",
    "cost_topics=cost_topics+cost_topics.T #sparse_ot is symmetric\n",
    "\n",
    "HOTT = np.zeros((bow_data.shape[0], bow_data.shape[0]))        \n",
    "for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        a,b,C=sparseOT(topic_proportions[i], topic_proportions[j], cost_topics)\n",
    "        HOTT[i,j]=ot.emd2(a,b,C)\n",
    "HOTT= HOTT+HOTT.T\n",
    "np.savetxt(\"result/WordMoverDistances/HOTT_\"+data_name+\".txt\",HOTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64fd8c-ff28-4eaf-9e83-d167db7352f3",
   "metadata": {},
   "source": [
    "# HROT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922f923-fbe7-40b6-bd8f-3bb011be386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Solver\n",
    "lam=[.1,.5,1,10]\n",
    "for l in lam:\n",
    "    WMDuot1=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    WMDuot2=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    #for i in range(bow_data.shape[0]):\n",
    "    #    print(i,end= \" \")\n",
    "    #    for j in range(i + 1, bow_data.shape[0]):\n",
    "    #        #print(j,end=\" \")\n",
    "    #        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "    #        _,_,Cx=sparseOT(bow_data[i], bow_data[i], cost_embeddings)\n",
    "    #        _,_,Cy=sparseOT(bow_data[j], bow_data[j], cost_embeddings)\n",
    "    #        P,Qx,Qy=UOT_W(a,b,C,lam=l,Cx=Cx,Cy=Cy,innerplan=True,solver=\"CLARABEL\")\n",
    "    #        WMDuot1[i,j]=np.sum(np.multiply(P,C))\n",
    "    #        WMDuot2[i,j]=np.sum(np.multiply(P,C))+l*(np.sum(Qx*Cx)+np.sum(Qy*Cy))\n",
    "    #WMDuot1 = WMDuot1 + WMDuot1.T\n",
    "    #WMDuot2 = WMDuot2 + WMDuot2.T\n",
    "    np.savetxt(\"result/WordMoverDistances/UOTP_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot1) #Only with the plan\n",
    "    np.savetxt(\"result/WordMoverDistances/UOT_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot2) #Plan and divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc9d46-7e21-4e41-9cb9-5c23028ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn Algorithm\n",
    "lam=[.1,.5,1,10]\n",
    "for l in lam:\n",
    "    WMDuot1=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    WMDuot2=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    for i in range(bow_data.shape[0]):\n",
    "        print(i, end=\" \")\n",
    "        for j in range(i + 1, bow_data.shape[0]):\n",
    "            #print(j,end=\" \")\n",
    "            a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "            print(bow_data[j].max())\n",
    "            _,_,Cx=sparseOT(bow_data[i], bow_data[i], cost_embeddings)\n",
    "            _,_,Cy=sparseOT(bow_data[j], bow_data[j], cost_embeddings)\n",
    "            print(Cy)\n",
    "            print(a.shape,b.shape,C.shape,Cy.shape,Cx.shape)\n",
    "            print(b)\n",
    "            P,Qx,Qy=sinkhorn(torch.tensor(a),torch.tensor(b),torch.tensor(C),lam=l,\n",
    "                             eps=3,numiter=20,lam2=None,pen=\"sinkhorn\",Cx=torch.tensor(Cx),\n",
    "                             Cy=torch.tensor(Cy),numiter2=1,innerplan=True)\n",
    "            P,Qx,Qy=P.numpy(),Qx.numpy(),Qy.numpy()\n",
    "            WMDuot1[i,j]=np.sum(np.multiply(P,C))\n",
    "            WMDuot2[i,j]=np.sum(np.multiply(P,C))+l*(np.sum(Qx*Cx)+np.sum(Qy*Cy))\n",
    "    WMDuot1 = WMDuot1 + WMDuot1.T\n",
    "    WMDuot2 = WMDuot2 + WMDuot2.T\n",
    "    np.savetxt(\"result/WordMoverDistances/UOTeP_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot1) #Only with the plan\n",
    "    np.savetxt(\"result/WordMoverDistances/UOTe_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot2) #Plan and divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c2055-e9e1-4428-9e22-b46b1846fc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
