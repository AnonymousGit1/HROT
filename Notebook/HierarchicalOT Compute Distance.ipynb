{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad16860-884c-40fb-b24f-b23e4994e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ot\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f88de5-95f5-4cbb-a222-036c40244dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')\n",
    "sys.path.append('../code/Latent_OT')\n",
    "from HierarchicalOT import load_wmd_data,change_embeddings,reduce_vocab,fit_topics,sparseOT\n",
    "from sinkhorn_iterates import sinkhorn\n",
    "from Latent_OT import LOT_fixsupp\n",
    "from linear_solver import UOT_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebd28d9-e7be-4354-8f64-6de34bde7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/WordMoverDistances/'\n",
    "embeddings_path = './data/WordMoverDistances/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# Pick a dataset (n_doc,n_vocab)\n",
    "#data_name = 'bbcsport-emd_tr_te_split.mat' #(737,3657)\n",
    "#data_name = 'twitter-emd_tr_te_split.mat' #(3108, 1205)\n",
    "#data_name = 'r8-emd_tr_te3.mat' # (7674,5495)\n",
    "#data_name = 'amazon-emd_tr_te_split.mat' #(8000, 16753)\n",
    "data_name = 'classic-emd_tr_te_split.mat' # (7093, 5813)\n",
    "#data_name = 'ohsumed-emd_tr_te_ix.mat'# (9152, 8261)\n",
    "\n",
    "vocab, embed_vocab, bow_data, y = load_wmd_data(data_path + data_name)\n",
    "y = y - 1\n",
    "vocab, embed_vocab, bow_data = change_embeddings(vocab, bow_data, embeddings_path) # embed in Glove\n",
    "\n",
    "vocab, embed_vocab, bow_data = reduce_vocab(bow_data, vocab, embed_vocab, embed_aggregate='mean') # Reduce vocabulary by removing short words, stop words, and stemming (root words?)\n",
    "embeddings = np.array([embed_vocab[w] for w in vocab])\n",
    "cost_embeddings = euclidean_distances(embeddings, embeddings) ** 2 # Matrix of word embeddings: nb_vocab x 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d76232-e350-46a4-bd56-199ecc71f0f3",
   "metadata": {},
   "source": [
    "# WMD Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf008f-2951-4ad2-99f2-8548e373f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UOT penalization\n",
    "WMD=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        #print(j,end=\" \")\n",
    "        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "        WMD[i,j]=ot.emd2(a,b,C)\n",
    "WMD = WMD + WMD.T\n",
    "np.savetxt(\"result/WordMoverDistances/WMD_\"+data_name+\".txt\",WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6141a23-0c24-4dcf-94f7-3168ebe49086",
   "metadata": {},
   "source": [
    "# HOTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a289f82-c26b-4e29-b257-20b279e74ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, lda_centers, topic_proportions = fit_topics(bow_data, embeddings, vocab, K=70)\n",
    "\n",
    "# Reduce topics to top-20 words via threshold\n",
    "# topics is now sparse\n",
    "n_words_keep = 20\n",
    "if n_words_keep is not None:\n",
    "    for k in range(70):\n",
    "        to_0_idx = np.argsort(-topics[k])[n_words_keep:]\n",
    "        topics[k][to_0_idx] = 0\n",
    "        \n",
    "# Compute WD for the unormalized topics[i],topics[j]. \n",
    "# It then defines cost matrix for distrib on topics\n",
    "cost_topics = np.zeros((topics.shape[0], topics.shape[0]))        \n",
    "for i in range(cost_topics.shape[0]):\n",
    "    for j in range(i + 1, cost_topics.shape[1]):\n",
    "        a,b,C=sparseOT(topics[i], topics[j], cost_embeddings)\n",
    "        cost_topics[i,j]=ot.emd2(a,b,C)\n",
    "cost_topics=cost_topics+cost_topics.T #sparse_ot is symmetric\n",
    "\n",
    "HOTT = np.zeros((bow_data.shape[0], bow_data.shape[0]))        \n",
    "for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        a,b,C=sparseOT(topic_proportions[i], topic_proportions[j], cost_topics)\n",
    "        HOTT[i,j]=ot.emd2(a,b,C)\n",
    "HOTT= HOTT+HOTT.T\n",
    "np.savetxt(\"result/WordMoverDistances/HOTT_\"+data_name+\".txt\",HOTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ceee9-5536-47c5-a082-c668b4c0a1fe",
   "metadata": {},
   "source": [
    "# Latent OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5666e228-f624-4124-8df9-910cf0709096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 7093\n",
      "INFO:lda:vocab_size: 5813\n",
      "INFO:lda:n_words: 371667\n",
      "INFO:lda:n_topics: 2\n",
      "INFO:lda:n_iter: 100\n",
      "INFO:lda:<0> log likelihood: -3056521\n",
      "INFO:lda:<10> log likelihood: -2794782\n",
      "INFO:lda:<20> log likelihood: -2718899\n",
      "INFO:lda:<30> log likelihood: -2704566\n",
      "INFO:lda:<40> log likelihood: -2696540\n",
      "INFO:lda:<50> log likelihood: -2692541\n",
      "INFO:lda:<60> log likelihood: -2690689\n",
      "INFO:lda:<70> log likelihood: -2690107\n",
      "INFO:lda:<80> log likelihood: -2689343\n",
      "INFO:lda:<90> log likelihood: -2689083\n",
      "INFO:lda:<99> log likelihood: -2688697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps=3\n",
    "\n",
    "topics, lda_centers, topic_proportions = fit_topics(bow_data, embeddings, vocab, K=2);\n",
    "n_words_keep = 20\n",
    "if n_words_keep is not None:\n",
    "    for k in range(2):\n",
    "        to_0_idx = np.argsort(-topics[k])[n_words_keep:]\n",
    "        topics[k][to_0_idx] = 0\n",
    "\n",
    "LOT1 = np.zeros((bow_data.shape[0], bow_data.shape[0]))        \n",
    "LOT2 = np.zeros((bow_data.shape[0], bow_data.shape[0]))        \n",
    "\"\"\"for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        ax,bx,Cx=sparseOT(bow_data[i], topics[0], cost_embeddings)\n",
    "        az,bz,Cz=sparseOT(topics[0], topics[1], cost_embeddings)\n",
    "        ay,by,Cy=sparseOT(topics[1], bow_data[j], cost_embeddings)\n",
    "        Gx = np.exp(-Cx / eps)\n",
    "        Gz = np.exp(-Cz / eps)\n",
    "        Gy = np.exp(-Cy / eps)\n",
    "\n",
    "        Px, Py, Pz, _=LOT_fixsupp(Gx, Gz, Gy,ax,by, niter=20)\n",
    "        LOT2[i,j]=np.sum(Pz*Cz)+np.sum(Px*Cx)+np.sum(Py*Cy)\n",
    "        LOT1[i,j]=np.sum(Pz*Cz)\n",
    "\n",
    "LOT1= LOT1+LOT1.T\n",
    "LOT2= LOT2+LOT2.T\"\"\"\n",
    "np.savetxt(\"result/WordMoverDistances/LOTP_\"+data_name+\".txt\",LOT1)\n",
    "np.savetxt(\"result/WordMoverDistances/LOT_\"+data_name+\".txt\",LOT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d5f3f-a2ad-4394-ab46-37218d903e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249f106b-4a5f-4348-9cd7-bd47e1d2f795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.21785160e-03, 1.83444373e-02, 9.21785160e-03, ...,\n",
       "       9.12658574e-05, 9.12658574e-05, 9.12658574e-05])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6997b58-349c-49c1-9e37-39a7dfdfd06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.21785160e-03, 1.83444373e-02, 9.21785160e-03, ...,\n",
       "        9.12658574e-05, 9.12658574e-05, 9.12658574e-05],\n",
       "       [8.50557115e-05, 8.50557115e-05, 2.56017692e-02, ...,\n",
       "        8.50557115e-05, 8.50557115e-05, 8.50557115e-05]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 3\n",
    "Cx,Cy,Cz=ot.dist(X,Zx),ot.dist(Zy,Y),ot.dist(Zx,Zy)\n",
    "Gx = np.exp(-Cx / eps)\n",
    "Gz = np.exp(-Cz / eps)\n",
    "Gy = np.exp(-Cy / eps)\n",
    "\n",
    "Px, Py, Pz, P=LOT_fixsupp(Gx, Gz, Gy,a,b, niter=20)\n",
    "print(np.sum(Px*Cx)+np.sum(Py*Cy)+np.sum(Pz*Cz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a4abd-1e0b-487d-b2c2-852360159638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_words_keep = 20\n",
    "if n_words_keep is not None:\n",
    "    for k in range(2):\n",
    "        to_0_idx = np.argsort(-topics[k])[n_words_keep:]\n",
    "        topics[k][to_0_idx] = 0\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ae3c0-2c4c-40c1-b50f-aeea0940cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce topics to top-20 words via threshold\n",
    "# topics is now sparse\n",
    "n_words_keep = 20\n",
    "if n_words_keep is not None:\n",
    "    for k in range(70):\n",
    "        to_0_idx = np.argsort(-topics[k])[n_words_keep:]\n",
    "        topics[k][to_0_idx] = 0\n",
    "        \n",
    "# Compute WD for the unormalized topics[i],topics[j]. \n",
    "# It then defines cost matrix for distrib on topics\n",
    "cost_topics = np.zeros((topics.shape[0], topics.shape[0]))        \n",
    "for i in range(cost_topics.shape[0]):\n",
    "    for j in range(i + 1, cost_topics.shape[1]):\n",
    "        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "        cost_topics[i,j]=ot.emd2(a,b,C)\n",
    "cost_topics=cost_topics+cost_topics.T #sparse_ot is symmetric\n",
    "\n",
    "HOTT = np.zeros((bow_data.shape[0], bow_data.shape[0]))        \n",
    "for i in range(bow_data.shape[0]):\n",
    "    print(i,end=\" \")\n",
    "    for j in range(i + 1, bow_data.shape[0]):\n",
    "        a,b,C=sparseOT(topic_proportions[i], topic_proportions[j], cost_topics)\n",
    "        HOTT[i,j]=ot.emd2(a,b,C)\n",
    "HOTT= HOTT+HOTT.T\n",
    "#np.savetxt(\"result/WordMoverDistances/HOTT_\"+data_name+\".txt\",HOTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64fd8c-ff28-4eaf-9e83-d167db7352f3",
   "metadata": {},
   "source": [
    "# HROT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922f923-fbe7-40b6-bd8f-3bb011be386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Solver\n",
    "lam=[.1,.5,1,10]\n",
    "for l in lam:\n",
    "    WMDuot1=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    WMDuot2=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    #for i in range(bow_data.shape[0]):\n",
    "    #    print(i,end= \" \")\n",
    "    #    for j in range(i + 1, bow_data.shape[0]):\n",
    "    #        #print(j,end=\" \")\n",
    "    #        a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "    #        _,_,Cx=sparseOT(bow_data[i], bow_data[i], cost_embeddings)\n",
    "    #        _,_,Cy=sparseOT(bow_data[j], bow_data[j], cost_embeddings)\n",
    "    #        P,Qx,Qy=UOT_W(a,b,C,lam=l,Cx=Cx,Cy=Cy,innerplan=True,solver=\"CLARABEL\")\n",
    "    #        WMDuot1[i,j]=np.sum(np.multiply(P,C))\n",
    "    #        WMDuot2[i,j]=np.sum(np.multiply(P,C))+l*(np.sum(Qx*Cx)+np.sum(Qy*Cy))\n",
    "    #WMDuot1 = WMDuot1 + WMDuot1.T\n",
    "    #WMDuot2 = WMDuot2 + WMDuot2.T\n",
    "    #np.savetxt(\"result/WordMoverDistances/UOTP_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot1) #Only with the plan\n",
    "    #np.savetxt(\"result/WordMoverDistances/UOT_\"+data_name+\"_\"+str(l)+\".txt\",WMDuot2) #Plan and divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc9d46-7e21-4e41-9cb9-5c23028ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn Algorithm\n",
    "lam=[.1]#[.1,.5,1,10]\n",
    "for l in lam:\n",
    "    WMDuot1=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    WMDuot2=np.zeros((bow_data.shape[0],bow_data.shape[0]))\n",
    "    for i in range(bow_data.shape[0]):\n",
    "        print(i, end=\" \")\n",
    "        for j in range(i + 1, bow_data.shape[0]):\n",
    "            #print(j,end=\" \")\n",
    "            a,b,C=sparseOT(bow_data[i], bow_data[j], cost_embeddings)\n",
    "            _,_,Cx=sparseOT(bow_data[i], bow_data[i], cost_embeddings)\n",
    "            _,_,Cy=sparseOT(bow_data[j], bow_data[j], cost_embeddings)\n",
    "            if (Cx.shape[0] == a.shape[0]) and (Cy.shape[0] == b.shape[0]):\n",
    "                P,Qx,Qy=sinkhorn(torch.tensor(a),torch.tensor(b),torch.tensor(C),lam=l,\n",
    "                             eps=3,numiter=20,lam2=None,pen=\"sinkhorn\",Cx=torch.tensor(Cx),\n",
    "                             Cy=torch.tensor(Cy),numiter2=1,innerplan=True)\n",
    "                P,Qx,Qy=P.numpy(),Qx.numpy(),Qy.numpy()\n",
    "                WMDuot1[i,j]=np.sum(np.multiply(P,C))\n",
    "                WMDuot2[i,j]=np.sum(np.multiply(P,C))+l*(np.sum(Qx*Cx)+np.sum(Qy*Cy))\n",
    "    WMDuot1 = WMDuot1 + WMDuot1.T\n",
    "    WMDuot2 = WMDuot2 + WMDuot2.T\n",
    "    #np.savetxt(\"result/WordMoverDistances/UOTeP_\"+data_name+\"_\"+str(10)+\".txt\",WMDuot1) #Only with the plan\n",
    "    #np.savetxt(\"result/WordMoverDistances/UOTe_\"+data_name+\"_\"+str(10)+\".txt\",WMDuot2) #Plan and divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497c2055-e9e1-4428-9e22-b46b1846fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900adaa-3b02-48eb-aff5-2ebc70012246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb25531-860a-4f7e-ad9c-bfe1c4c771f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ui\n"
     ]
    }
   ],
   "source": [
    "x=5\n",
    "print('ui')\n",
    "with suppress_stdout():\n",
    "    x+=5\n",
    "    print(\"Now you don't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eaf5782-4d6b-46f8-a807-123c4fc45798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734286c2-043b-41b2-94cd-8ea907c51280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
